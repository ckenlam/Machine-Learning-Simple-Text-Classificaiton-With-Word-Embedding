{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cover.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recent project at my work seeks to explore the possiblity of detecting sales potential in messages from customer support's live chat (i.e binary classification: sales or non-sales potential). Essentially, it is a text classification problem which could be solved by using word embedding. While I did not have sufficient data to conduct such experiment, I still wanted to understand how a pre-trained word embedding model could make training an accurate text classification model possible.\n",
    "\n",
    "The idea of word embedding is to capture the context of a word in a document, semantic and syntactic similarity, relation with other words. While it doesn't \"undestand\" the meaning of a word the same way human would, it assigns close spatial positions to words with similar context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenlam/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import punkt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GloVe embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe vs word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word2vec** is a predictive model, meaning that it trains by trying to predict a target word given a context (CBOW) or the context words from the target (skip-gram). The model uses trainable embedding weights to map words to their corresponding embeddings, which are used to help the model make predictions.\n",
    "\n",
    "The **GloVe model** uses a co-occurence counts matrix to make the embeddings. Each row of the matrix represents a word, while each column represents the contexts that words can appear in. The matrix values represent the frequency a word appears in a given context. Then, dimensionality reduction is applied to this matrix to create the resulting embedding matrix (each row will be a wordâ€™s embedding vector).\n",
    "\n",
    "What I am importing here is a **pre-trained GloVe by Stanford NLP Group**. It was trained on data of Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the GloVe vectors used in this notebook [here](http://nlp.stanford.edu/data/glove.840B.300d.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [03:05, 11837.83it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt', encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "       coefs = np.asarray(values[1:], dtype='float32')\n",
    "       embeddings_index[word] = coefs\n",
    "    except ValueError:\n",
    "       pass\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive/Negative Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, I'm using the [Sentiment Labelled Sentences Dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#) from UC Irvine Machine Learning Repository. It contains sentences labelled with positive/negative sentiment. I will train a sentiment classification model with this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('./reviews/amazon_cells_labelled.txt', sep=\"\\t\", header=None)\n",
    "data2 = pd.read_csv('./reviews/imdb_labelled.txt', sep=\"\\t\", header=None)\n",
    "data3 = pd.read_csv('./reviews/yelp_labelled.txt', sep=\"\\t\", header=None)\n",
    "data1.columns = [\"review\", 'positive']\n",
    "data2.columns = [\"review\", 'positive']\n",
    "data3.columns = [\"review\", 'positive']\n",
    "data= pd.concat([data1,data2,data3])\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  positive\n",
       "0  So there is no way for me to plug it in here i...         0\n",
       "1                        Good case, Excellent value.         1\n",
       "2                             Great for the jawbone.         1\n",
       "3  Tied to charger for conversations lasting more...         0\n",
       "4                                  The mic is great.         1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2748 reviews in this dataset\n"
     ]
    }
   ],
   "source": [
    "print('There are %s reviews in this dataset' %(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.504367\n",
       "0    0.495633\n",
       "Name: positive, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "class_names = ['positive']\n",
    "train = data\n",
    "train.positive.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all, test_all = train_test_split(train,test_size=0.20, random_state=42)\n",
    "train_text = train_all['review']\n",
    "test_text = test_all['review']\n",
    "\n",
    "all_text = pd.concat([train_text, test_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "# reference: https://stackoverflow.com/questions/30795944/how-can-a-sentence-or-a-document-be-converted-to-a-vector\n",
    "# reference: https://github.com/abhishekkrthakur/is_that_a_duplicate_quora_question/blob/master/feature_engineering.py\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    \n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2198/2198 [00:00<00:00, 3105.87it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:00<00:00, 3838.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(train_text)]\n",
    "xtest_glove = [sent2vec(x) for x in tqdm(test_text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xtest_glove = np.array(xtest_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class positive is 0.8861175709054306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenlam/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "result = pd.DataFrame.from_dict({'id': test_all.index})\n",
    "\n",
    "train_target = train_all[class_name]\n",
    "classifier = XGBClassifier(n_estimators=400, random_state = 0)\n",
    "\n",
    "cv_score = np.mean(cross_val_score(classifier, xtrain_glove, train_target, cv=3, scoring='roc_auc'))\n",
    "scores.append(cv_score)\n",
    "print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "classifier.fit(xtrain_glove, train_target)\n",
    "result[class_name] = classifier.predict(xtest_glove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 0.998\n",
      "Test Accuracy = 0.833\n",
      "ROC_AUC_score : 0.835131\n",
      "[[231  60]\n",
      " [ 32 227]]\n",
      "--------------- CLASSIFICATION REPORT ---------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.79      0.83       291\n",
      "          1       0.79      0.88      0.83       259\n",
      "\n",
      "avg / total       0.84      0.83      0.83       550\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenlam/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kenlam/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "print(\"Training Accuracy = {:.3f}\".format(classifier.score(xtrain_glove, train_target)))\n",
    "\n",
    "print(\"Test Accuracy = {:.3f}\".format(classifier.score(xtest_glove, test_all[class_name])))\n",
    "print(\"ROC_AUC_score : %.6f\" % (roc_auc_score(test_all[class_name], result[class_name])))\n",
    "\n",
    "#Confusion Matrix\n",
    "print(confusion_matrix(test_all[class_name], result[class_name]))\n",
    "print(\"-\"*15,\"CLASSIFICATION REPORT\",\"-\"*15)\n",
    "print(classification_report(test_all[class_name], result[class_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note**:\n",
    "\n",
    "ROC-AUC tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. Essentially, higher the AUC, better the model is at distinguishing between texts with positive review and negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>positive</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2516</td>\n",
       "      <td>It's close to my house, it's low-key, non-fanc...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2642</td>\n",
       "      <td>If you stay in Vegas you must get breakfast he...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1359</td>\n",
       "      <td>Let's start with all the problemsÂ—the acting, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1702</td>\n",
       "      <td>It's too bad that everyone else involved didn'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2660</td>\n",
       "      <td>i felt insulted and disrespected, how could yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>564</td>\n",
       "      <td>Yet Plantronincs continues to use the same fla...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1330</td>\n",
       "      <td>Whatever prompted such a documentary is beyond...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2375</td>\n",
       "      <td>Any grandmother can make a roasted chicken bet...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>695</td>\n",
       "      <td>Do NOT buy if you want to use the holster.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>321</td>\n",
       "      <td>I ordered this product first and was unhappy w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>990</td>\n",
       "      <td>I'm really disappointed all I have now is a ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1088</td>\n",
       "      <td>The only possible way this movie could be rede...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>783</td>\n",
       "      <td>It was horrible!.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>322</td>\n",
       "      <td>The Ngage is still lacking in earbuds.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2089</td>\n",
       "      <td>Of all the dishes, the salmon was the best, bu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2381</td>\n",
       "      <td>High-quality chicken on the chicken Caesar salad.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>527</td>\n",
       "      <td>The noise shield is incrediable.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1912</td>\n",
       "      <td>Restaurant is always full but never a wait.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>239</td>\n",
       "      <td>Do not make the same mistake as me.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1406</td>\n",
       "      <td>To be honest with you, this is unbelievable no...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2122</td>\n",
       "      <td>And the beans and rice were mediocre at best.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>194</td>\n",
       "      <td>Not impressed.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>831</td>\n",
       "      <td>All three broke within two months of use.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2292</td>\n",
       "      <td>No, I'm going to eat the potato that I found s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1234</td>\n",
       "      <td>In short - this was a monumental waste of time...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2734</td>\n",
       "      <td>Shrimp- When I unwrapped it (I live only 1/2 a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>439</td>\n",
       "      <td>If you don't find it, too bad, as again the un...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1818</td>\n",
       "      <td>The deal included 5 tastings and 2 drinks, and...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>841</td>\n",
       "      <td>the only VERY DISAPPOINTING thing was there wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>233</td>\n",
       "      <td>Great sound and service.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>2453</td>\n",
       "      <td>I work in the hospitality industry in Paradise...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>765</td>\n",
       "      <td>in addition it feels &amp;amp; looks as if the pho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>2132</td>\n",
       "      <td>WILL NEVER EVER GO BACK AND HAVE TOLD MANY PEO...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>1913</td>\n",
       "      <td>DELICIOUS!!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>610</td>\n",
       "      <td>It doesn't work in Europe or Asia.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>942</td>\n",
       "      <td>The mic there is a joke, and the volume is qui...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>1247</td>\n",
       "      <td>The acting was decidely wooden, though no wors...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>945</td>\n",
       "      <td>It is easy to turn on and off when you are in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>2195</td>\n",
       "      <td>Sauce was tasteless.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>461</td>\n",
       "      <td>We would recommend these to others.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>1135</td>\n",
       "      <td>You'll love it!  \\t1\\nThis movie is BAD.  \\t0\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>157</td>\n",
       "      <td>This item is fantastic and works perfectly!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>1624</td>\n",
       "      <td>It's a long time since I was so entertained by...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>1425</td>\n",
       "      <td>It's as continuously beautiful to look at as a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>238</td>\n",
       "      <td>I was very excited to get this headset because...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>1910</td>\n",
       "      <td>Their menu is diverse, and reasonably priced.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>2140</td>\n",
       "      <td>I have been here several times in the past, an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>2345</td>\n",
       "      <td>It'll be a regular stop on my trips to Phoenix!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>100</td>\n",
       "      <td>Integrated seamlessly with the Motorola RAZR p...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>315</td>\n",
       "      <td>It's uncomfortable and the sound quality is qu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1971</td>\n",
       "      <td>Hopefully this bodes for them going out of bus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>2424</td>\n",
       "      <td>I can't wait to go back.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>599</td>\n",
       "      <td>For the price on Amazon, it is an excellent pr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1952</td>\n",
       "      <td>Service is friendly and inviting.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>261</td>\n",
       "      <td>Only had this a month but it's worked flawless...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>644</td>\n",
       "      <td>I contacted the company and they told me that,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>1399</td>\n",
       "      <td>Every single character was hilarious and deser...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1779</td>\n",
       "      <td>This was like the final blow!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>719</td>\n",
       "      <td>if you simply want a small flip phone -- look ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>179</td>\n",
       "      <td>If you like a loud buzzing to override all you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>550 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                             review  positive  \\\n",
       "0     2516  It's close to my house, it's low-key, non-fanc...         1   \n",
       "1     2642  If you stay in Vegas you must get breakfast he...         1   \n",
       "2     1359  Let's start with all the problemsÂ—the acting, ...         0   \n",
       "3     1702  It's too bad that everyone else involved didn'...         0   \n",
       "4     2660  i felt insulted and disrespected, how could yo...         0   \n",
       "5      564  Yet Plantronincs continues to use the same fla...         0   \n",
       "6     1330  Whatever prompted such a documentary is beyond...         0   \n",
       "7     2375  Any grandmother can make a roasted chicken bet...         0   \n",
       "8      695         Do NOT buy if you want to use the holster.         0   \n",
       "9      321  I ordered this product first and was unhappy w...         0   \n",
       "10     990  I'm really disappointed all I have now is a ch...         0   \n",
       "11    1088  The only possible way this movie could be rede...         0   \n",
       "12     783                                  It was horrible!.         0   \n",
       "13     322             The Ngage is still lacking in earbuds.         0   \n",
       "14    2089  Of all the dishes, the salmon was the best, bu...         1   \n",
       "15    2381  High-quality chicken on the chicken Caesar salad.         1   \n",
       "16     527                   The noise shield is incrediable.         1   \n",
       "17    1912        Restaurant is always full but never a wait.         1   \n",
       "18     239                Do not make the same mistake as me.         0   \n",
       "19    1406  To be honest with you, this is unbelievable no...         0   \n",
       "20    2122      And the beans and rice were mediocre at best.         0   \n",
       "21     194                                     Not impressed.         0   \n",
       "22     831          All three broke within two months of use.         0   \n",
       "23    2292  No, I'm going to eat the potato that I found s...         0   \n",
       "24    1234  In short - this was a monumental waste of time...         0   \n",
       "25    2734  Shrimp- When I unwrapped it (I live only 1/2 a...         0   \n",
       "26     439  If you don't find it, too bad, as again the un...         0   \n",
       "27    1818  The deal included 5 tastings and 2 drinks, and...         1   \n",
       "28     841  the only VERY DISAPPOINTING thing was there wa...         0   \n",
       "29     233                           Great sound and service.         1   \n",
       "..     ...                                                ...       ...   \n",
       "520   2453  I work in the hospitality industry in Paradise...         0   \n",
       "521    765  in addition it feels &amp; looks as if the pho...         0   \n",
       "522   2132  WILL NEVER EVER GO BACK AND HAVE TOLD MANY PEO...         0   \n",
       "523   1913                                        DELICIOUS!!         1   \n",
       "524    610                 It doesn't work in Europe or Asia.         0   \n",
       "525    942  The mic there is a joke, and the volume is qui...         0   \n",
       "526   1247  The acting was decidely wooden, though no wors...         0   \n",
       "527    945  It is easy to turn on and off when you are in ...         1   \n",
       "528   2195                               Sauce was tasteless.         0   \n",
       "529    461                We would recommend these to others.         1   \n",
       "530   1135  You'll love it!  \\t1\\nThis movie is BAD.  \\t0\\...         1   \n",
       "531    157        This item is fantastic and works perfectly!         1   \n",
       "532   1624  It's a long time since I was so entertained by...         1   \n",
       "533   1425  It's as continuously beautiful to look at as a...         1   \n",
       "534    238  I was very excited to get this headset because...         1   \n",
       "535   1910      Their menu is diverse, and reasonably priced.         1   \n",
       "536   2140  I have been here several times in the past, an...         1   \n",
       "537   2345    It'll be a regular stop on my trips to Phoenix!         1   \n",
       "538    100  Integrated seamlessly with the Motorola RAZR p...         1   \n",
       "539    315  It's uncomfortable and the sound quality is qu...         0   \n",
       "540   1971  Hopefully this bodes for them going out of bus...         0   \n",
       "541   2424                           I can't wait to go back.         1   \n",
       "542    599  For the price on Amazon, it is an excellent pr...         1   \n",
       "543   1952                  Service is friendly and inviting.         1   \n",
       "544    261  Only had this a month but it's worked flawless...         1   \n",
       "545    644  I contacted the company and they told me that,...         0   \n",
       "546   1399  Every single character was hilarious and deser...         1   \n",
       "547   1779                      This was like the final blow!         0   \n",
       "548    719  if you simply want a small flip phone -- look ...         0   \n",
       "549    179  If you like a loud buzzing to override all you...         0   \n",
       "\n",
       "     prediction  \n",
       "0             1  \n",
       "1             0  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  \n",
       "5             1  \n",
       "6             0  \n",
       "7             1  \n",
       "8             1  \n",
       "9             0  \n",
       "10            0  \n",
       "11            0  \n",
       "12            0  \n",
       "13            0  \n",
       "14            1  \n",
       "15            1  \n",
       "16            1  \n",
       "17            1  \n",
       "18            0  \n",
       "19            0  \n",
       "20            0  \n",
       "21            1  \n",
       "22            0  \n",
       "23            0  \n",
       "24            0  \n",
       "25            0  \n",
       "26            0  \n",
       "27            1  \n",
       "28            0  \n",
       "29            1  \n",
       "..          ...  \n",
       "520           1  \n",
       "521           1  \n",
       "522           0  \n",
       "523           1  \n",
       "524           1  \n",
       "525           0  \n",
       "526           0  \n",
       "527           1  \n",
       "528           0  \n",
       "529           0  \n",
       "530           1  \n",
       "531           1  \n",
       "532           1  \n",
       "533           1  \n",
       "534           1  \n",
       "535           1  \n",
       "536           1  \n",
       "537           1  \n",
       "538           1  \n",
       "539           0  \n",
       "540           0  \n",
       "541           0  \n",
       "542           1  \n",
       "543           1  \n",
       "544           1  \n",
       "545           0  \n",
       "546           1  \n",
       "547           0  \n",
       "548           0  \n",
       "549           0  \n",
       "\n",
       "[550 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns=['id','prediction']\n",
    "test_merge = test_all.reset_index(drop=False)\n",
    "pd.concat([test_merge,result['prediction']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without doing any hyper-parameters tuning, I was able to obtain very decent results with the pre-trained GloVe model and out-of-the-box XGBoost algorithm. This simple experiment shows how powerful word embedding can be in retaining  the semantics of words. This methodology will be a good place to start for my initial classification problem (i.e. sales/non-sales) once I have collected an adequate amount of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
